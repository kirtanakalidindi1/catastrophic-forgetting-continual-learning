{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "mBIcZ5KhPccs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "NSAMPLES = 500\n",
        "# Create toy Gaussian data\n",
        "def create_gaussian_data(mean, std, n_samples):\n",
        "    return np.random.randn(n_samples, 2) * std + mean\n",
        "\n",
        "# Dataset class\n",
        "class GaussianToyDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create complex multi-cluster Gaussian data\n",
        "def create_complex_data(n_samples_per_cluster=100, n_clusters_per_class=3, std=0.5):\n",
        "    X = []\n",
        "    y = []\n",
        "    centers_class0 = np.random.randn(n_clusters_per_class, 2) * 3.0  # Spread out\n",
        "    centers_class1 = np.random.randn(n_clusters_per_class, 2) * 3.0 + 5.0  # Move class 1 far away\n",
        "\n",
        "    for center in centers_class0:\n",
        "        points = np.random.randn(n_samples_per_cluster, 2) * std + center\n",
        "        X.append(points)\n",
        "        y += [0] * n_samples_per_cluster\n",
        "\n",
        "    for center in centers_class1:\n",
        "        points = np.random.randn(n_samples_per_cluster, 2) * std + center\n",
        "        X.append(points)\n",
        "        y += [1] * n_samples_per_cluster\n",
        "\n",
        "    X = np.vstack(X)\n",
        "    y = np.array(y)\n",
        "    return X, y\n",
        "\n",
        "def nonlinear_warp(X, freq=2.0, amp=0.5):\n",
        "    X_new = X.copy()\n",
        "    X_new[:, 0] += amp * np.sin(freq * X[:, 1])\n",
        "    X_new[:, 1] += amp * np.sin(freq * X[:, 0])\n",
        "    return X_new\n",
        "\n",
        "def add_moderate_label_noise(y, noise_rate=0.2):\n",
        "    noisy_y = y.copy()\n",
        "    flip_mask = np.random.rand(len(y)) < noise_rate\n",
        "    noisy_y[flip_mask] = 1 - noisy_y[flip_mask]  # assuming binary\n",
        "    return noisy_y\n",
        "\n",
        "# SIMPLE DATA CREATION\n",
        "\n",
        "# Task A (original)\n",
        "# X0 = create_gaussian_data([-1, -1], 1, NSAMPLES)\n",
        "# X1 = create_gaussian_data([1, 1], 1, NSAMPLES)\n",
        "# X_A = np.vstack((X0, X1))\n",
        "# y_A = np.array([0]*NSAMPLES + [1]*NSAMPLES)\n",
        "\n",
        "# Task B (rotation)\n",
        "# theta = np.pi / 4  # 45 degrees\n",
        "# R = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
        "# X_B = X_A @ R.T\n",
        "# y_B = y_A.copy()\n",
        "\n",
        "\n",
        "\n",
        "NSAMPLES = 500\n",
        "\n",
        "# Task A (Simple Gaussian)\n",
        "X0 = create_gaussian_data([-1, -1], 1, NSAMPLES)\n",
        "X1 = create_gaussian_data([1, 1], 1, NSAMPLES)\n",
        "X_A = np.vstack((X0, X1))\n",
        "y_A = np.array([0]*NSAMPLES + [1]*NSAMPLES)\n",
        "\n",
        "# Task B (Rotated Simple Gaussian with minor label noise)\n",
        "theta = np.pi / 4  # 45 degrees\n",
        "R = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
        "X_B = X_A @ R.T\n",
        "y_B = y_A.copy()\n",
        "# y_B = add_moderate_label_noise(y_B, noise_rate=0.01)\n",
        "\n",
        "# Task C (Multi Blob Gaussian with more blobs and more label noise)\n",
        "X_C, y_C = create_complex_data(NSAMPLES // 5, n_clusters_per_class=5)\n",
        "# y_C = add_moderate_label_noise(y_C, noise_rate=0.1)\n",
        "\n",
        "\n",
        "\n",
        "trainset_A = GaussianToyDataset(X_A, y_A)\n",
        "trainset_B = GaussianToyDataset(X_B, y_B)\n",
        "trainset_C = GaussianToyDataset(X_C, y_C)\n",
        "\n",
        "trainloader_A = DataLoader(trainset_A, batch_size=64, shuffle=True)\n",
        "trainloader_B = DataLoader(trainset_B, batch_size=64, shuffle=True)\n",
        "trainloader_C = DataLoader(trainset_C, batch_size=64, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "W59mTXWDTJCo"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(2, 16)   # First hidden layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(16, 2)   # Output layer (for 2 classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "UEqZSq_bVb69"
      },
      "outputs": [],
      "source": [
        "# Define the prior and posterior distributions\n",
        "def prior_distribution(model):\n",
        "    return [param.data.clone() for param in model.parameters()]\n",
        "\n",
        "def posterior_distribution(model):\n",
        "    return [param.data.clone() for param in model.parameters()]\n",
        "\n",
        "def kl_divergence(prior, posterior, sigma_sq=1.0):\n",
        "    # we don't have access to a \"distribution\", therefore, we assume both the prior and the posterior have some shared covariance matrix\n",
        "    kl = 0.0\n",
        "    for p, q in zip(prior, posterior):\n",
        "        kl += torch.sum((q - p) ** 2)\n",
        "    return (0.5 / sigma_sq) * kl\n",
        "\n",
        "def pac_bayes_bound(prior, posterior, n_samples, empirical_loss, delta=0.05, sigma_sq=1.0):\n",
        "    kl = kl_divergence(prior, posterior, sigma_sq=sigma_sq)\n",
        "    bound_term = (kl + np.log(2 * np.sqrt(n_samples) / delta)) / (2 * n_samples)\n",
        "    return empirical_loss + torch.sqrt(torch.tensor(bound_term, dtype=torch.float32))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "ZeV2vTTtTMSx"
      },
      "outputs": [],
      "source": [
        "def train(model, loader, optimizer, criterion, epochs, n_samples):\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "def train_with_pacbayes(model, loader, optimizer, criterion, epochs, n_samples, prior):\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            posterior = posterior_distribution(model)\n",
        "            epsilon = total_loss / len(loader)\n",
        "            bound = pac_bayes_bound(prior, posterior, n_samples, epsilon)\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}, PAC-Bayes Bound: {bound:.4f}\")\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            preds = model(x).argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += len(y)\n",
        "    return correct / total * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im_02vksTORU",
        "outputId": "d4df9eaf-27b2-4060-aba7-78cca8a986a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on Task A (Original Data)\n",
            "Epoch 10/100, Loss: 0.2653\n",
            "Epoch 20/100, Loss: 0.2014\n",
            "Epoch 30/100, Loss: 0.2965\n",
            "Epoch 40/100, Loss: 0.2704\n",
            "Epoch 50/100, Loss: 0.1931\n",
            "Epoch 60/100, Loss: 0.1577\n",
            "Epoch 70/100, Loss: 0.2005\n",
            "Epoch 80/100, Loss: 0.1188\n",
            "Epoch 90/100, Loss: 0.2333\n",
            "Epoch 100/100, Loss: 0.0326\n",
            "Task A Accuracy: 92.20%, Task B Before Training: 84.10%\n",
            "\n",
            "Training on Task B (Rotated Data, simple transformation)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2253852/1029784997.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return empirical_loss + torch.sqrt(torch.tensor(bound_term, dtype=torch.float32))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/100, Loss: 0.2391, PAC-Bayes Bound: 0.3239\n",
            "Epoch 20/100, Loss: 0.2650, PAC-Bayes Bound: 0.2966\n",
            "Epoch 30/100, Loss: 0.1657, PAC-Bayes Bound: 0.2851\n",
            "Epoch 40/100, Loss: 0.0715, PAC-Bayes Bound: 0.2793\n",
            "Epoch 50/100, Loss: 0.2590, PAC-Bayes Bound: 0.2823\n",
            "Epoch 60/100, Loss: 0.2234, PAC-Bayes Bound: 0.2808\n",
            "Epoch 70/100, Loss: 0.1059, PAC-Bayes Bound: 0.2777\n",
            "Epoch 80/100, Loss: 0.2235, PAC-Bayes Bound: 0.2804\n",
            "Epoch 90/100, Loss: 0.0914, PAC-Bayes Bound: 0.2772\n",
            "Epoch 100/100, Loss: 0.2490, PAC-Bayes Bound: 0.2809\n",
            "After Task B → Task A Accuracy: 84.50%, Task B Accuracy: 92.10%\n",
            "\n",
            "Training on Task C (Complex Multi-Blob Gaussian, more difficult transformation for the model to forget on)\n",
            "Epoch 10/100, Loss: 0.3193, PAC-Bayes Bound: 0.4955\n",
            "Epoch 20/100, Loss: 0.3210, PAC-Bayes Bound: 0.4384\n",
            "Epoch 30/100, Loss: 0.4041, PAC-Bayes Bound: 0.4251\n",
            "Epoch 40/100, Loss: 0.2753, PAC-Bayes Bound: 0.4126\n",
            "Epoch 50/100, Loss: 0.3316, PAC-Bayes Bound: 0.4068\n",
            "Epoch 60/100, Loss: 0.4370, PAC-Bayes Bound: 0.4038\n",
            "Epoch 70/100, Loss: 0.2649, PAC-Bayes Bound: 0.3953\n",
            "Epoch 80/100, Loss: 0.2892, PAC-Bayes Bound: 0.3921\n",
            "Epoch 90/100, Loss: 0.3277, PAC-Bayes Bound: 0.3902\n",
            "Epoch 100/100, Loss: 0.3257, PAC-Bayes Bound: 0.3877\n",
            "After Task C → Task A Accuracy: 63.80%, Task B Accuracy: 63.70%, Task C Accuracy: 82.90%\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1984)\n",
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"Training on Task A (Original Data)\")\n",
        "train(model, trainloader_A, optimizer, criterion, 100, NSAMPLES)\n",
        "acc_A_before = evaluate(model, trainloader_A)\n",
        "acc_B_before = evaluate(model, trainloader_B)\n",
        "print(f\"Task A Accuracy: {acc_A_before:.2f}%, Task B Before Training: {acc_B_before:.2f}%\")\n",
        "\n",
        "prior = prior_distribution(model)\n",
        "print(\"\\nTraining on Task B (Rotated Data, simple transformation)\")\n",
        "train_with_pacbayes(model, trainloader_B, optimizer, criterion, 100, NSAMPLES, prior)\n",
        "acc_A_after = evaluate(model, trainloader_A)\n",
        "acc_B_after = evaluate(model, trainloader_B)\n",
        "print(f\"After Task B → Task A Accuracy: {acc_A_after:.2f}%, Task B Accuracy: {acc_B_after:.2f}%\")\n",
        "\n",
        "print(\"\\nTraining on Task C (Complex Multi-Blob Gaussian, more difficult transformation for the model to forget on)\")\n",
        "train_with_pacbayes(model, trainloader_C, optimizer, criterion, 100, NSAMPLES, prior)\n",
        "acc_A_after = evaluate(model, trainloader_A)\n",
        "acc_B_after = evaluate(model, trainloader_B)\n",
        "acc_C_after = evaluate(model, trainloader_C)\n",
        "print(f\"After Task C → Task A Accuracy: {acc_A_after:.2f}%, Task B Accuracy: {acc_B_after:.2f}%, Task C Accuracy: {acc_C_after:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxcevFohno-o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
