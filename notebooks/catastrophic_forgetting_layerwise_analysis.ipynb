{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2mZmX91zNyAz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "device = 'cuda'\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANALYSIS"
      ],
      "metadata": {
        "id": "bdGrz1ZnaOIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.linalg import svdvals\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cross_decomposition import CCA\n",
        "import seaborn as sns\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "NSAMPLES = 2000\n",
        "num_classes = 4  # Set the number of classes\n",
        "\n",
        "# Dataset class\n",
        "class ComplexGaussianToyDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create complex multi-cluster Gaussian data with variable number of classes\n",
        "def create_complex_data(n_samples_per_cluster=100, n_clusters_per_class=3, std=0.5, num_classes=2, seed=None):\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "    X = []\n",
        "    y = []\n",
        "    centers = []\n",
        "    for c in range(num_classes):\n",
        "        class_centers = np.random.randn(n_clusters_per_class, 2) * (3.0 + c * 1.5) # Spread out classes more\n",
        "        centers.append(class_centers)\n",
        "        for center in class_centers:\n",
        "            points = np.random.randn(n_samples_per_cluster, 2) * (std + c * 0.1) + center # Vary std per class\n",
        "            X.append(points)\n",
        "            y += [c] * n_samples_per_cluster\n",
        "\n",
        "    X = np.vstack(X)\n",
        "    y = np.array(y)\n",
        "    return X, y\n",
        "\n",
        "def nonlinear_warp(X, freq=2.0, amp=0.5):\n",
        "    X_new = X.copy()\n",
        "    X_new[:, 0] += amp * np.sin(freq * X[:, 1])\n",
        "    X_new[:, 1] += amp * np.cos(freq * X[:, 0] + np.pi / 4) # Different warp\n",
        "    return X_new\n",
        "\n",
        "def rotate_data(X, angle_degrees):\n",
        "    angle_radians = np.radians(angle_degrees)\n",
        "    rotation_matrix = np.array([[np.cos(angle_radians), -np.sin(angle_radians)],\n",
        "                                [np.sin(angle_radians), np.cos(angle_radians)]])\n",
        "    return X @ rotation_matrix.T\n",
        "\n",
        "# DATA CREATION\n",
        "num_datasets = 9 # Toggle the number of datasets to use\n",
        "num_complex_datasets = 15\n",
        "complex_datasets = []\n",
        "seeds = np.random.randint(0, 1000, num_complex_datasets)\n",
        "\n",
        "for i in range(num_complex_datasets):\n",
        "    n_clusters = np.random.randint(2, 5)\n",
        "    std_dev = np.random.uniform(0.4, 0.8)\n",
        "    X_base, y = create_complex_data(NSAMPLES // (num_classes * n_clusters), n_clusters, std_dev, num_classes, seed=seeds[i])\n",
        "\n",
        "    # Introduce more diversity between datasets\n",
        "    if i % 4 == 0:\n",
        "        X = X_base\n",
        "    elif i % 4 == 1:\n",
        "        X = nonlinear_warp(X_base, freq=1.5 + i * 0.2, amp=0.6 + i * 0.1)\n",
        "    elif i % 4 == 2:\n",
        "        X = rotate_data(X_base, angle_degrees=i * 15)\n",
        "    else:\n",
        "        X = nonlinear_warp(rotate_data(X_base, angle_degrees=-i * 10), freq=2.0 - i * 0.1, amp=0.5 + i * 0.05)\n",
        "\n",
        "    # Introduce label shifts for more forgetting\n",
        "    if i > 0 and i % 3 == 0:\n",
        "        y = (y + 1) % num_classes # Shift labels\n",
        "\n",
        "    complex_datasets.append(ComplexGaussianToyDataset(X, y))\n",
        "\n",
        "# Select the number of datasets to use\n",
        "selected_datasets = complex_datasets[:num_datasets]\n",
        "train_loaders = [DataLoader(d, batch_size=64, shuffle=True) for d in selected_datasets]\n",
        "\n",
        "# Define the prior and posterior distributions\n",
        "def prior_distribution(model):\n",
        "    return [param.data.clone() for param in model.parameters()]\n",
        "\n",
        "def posterior_distribution(model):\n",
        "    return [param.data.clone() for param in model.parameters()]\n",
        "\n",
        "def kl_divergence(prior, posterior, sigma_sq=1.0):\n",
        "    # we don't have access to a \"distribution\", therefore, we assume both the prior and the posterior have some shared covariance matrix\n",
        "    kl = 0.0\n",
        "    for p, q in zip(prior, posterior):\n",
        "        kl += torch.sum((q - p) ** 2)\n",
        "    return (0.5 / sigma_sq) * kl\n",
        "\n",
        "def pac_bayes_bound(prior, posterior, n_samples, empirical_loss, delta=0.05, sigma_sq=1.0):\n",
        "    kl = kl_divergence(prior, posterior, sigma_sq=sigma_sq)\n",
        "    bound_term = (kl + np.log(2 * np.sqrt(n_samples) / delta)) / (2 * n_samples)\n",
        "    return empirical_loss + torch.sqrt(torch.tensor(bound_term, dtype=torch.float32))\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "          nn.Linear(2, 32),   # Increased hidden units\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(32, num_classes)   # Output layer for num_classes\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class EWC:\n",
        "    def __init__(self, model: nn.Module, dataloader, device='cuda:0'):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.model.eval()\n",
        "        self.params = {n: p.clone().detach().to(self.device) for n, p in self.model.named_parameters() if p.requires_grad}\n",
        "        self.fisher = self._compute_fisher(dataloader)\n",
        "\n",
        "    def _compute_fisher(self, dataloader):\n",
        "        fisher = {n: torch.zeros_like(p, device=self.device) for n, p in self.model.named_parameters() if p.requires_grad}\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        for inputs, labels in dataloader:\n",
        "            self.model.zero_grad()\n",
        "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "            outputs = self.model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            for n, p in self.model.named_parameters():\n",
        "                if p.grad is not None:\n",
        "                    fisher[n] += p.grad.data.pow(2)\n",
        "\n",
        "        for n in fisher:\n",
        "            fisher[n] /= len(dataloader)\n",
        "\n",
        "        return fisher\n",
        "\n",
        "    def penalty(self, model: nn.Module):\n",
        "        loss = 0\n",
        "        for n, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                _loss = self.fisher[n] * (p - self.params[n]).pow(2)\n",
        "                loss += _loss.sum()\n",
        "        return loss\n",
        "\n",
        "\n",
        "def train(model, loader, optimizer, criterion, epochs, n_samples, ewc = None, use_ewc = True):\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            if use_ewc:\n",
        "              lam = 40\n",
        "\n",
        "              ewc_penalty = 0\n",
        "              for ewc_instance in ewc:  # Iterate over EWC instances\n",
        "                  ewc_penalty += ewc_instance.penalty(model)\n",
        "              loss += lam * ewc_penalty\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "def train_with_pacbayes(model, loader, optimizer, criterion, epochs, n_samples, prior):\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            posterior = posterior_distribution(model)\n",
        "            epsilon = total_loss / len(loader)\n",
        "            bound = pac_bayes_bound(prior, posterior, n_samples, epsilon)\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}, PAC-Bayes Bound: {bound:.4f}\")\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            preds = model(x).argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += len(y)\n",
        "    return correct / total * 100\n",
        "\n",
        "import copy\n",
        "\n",
        "torch.manual_seed(1984)\n",
        "saved_models_list = []\n",
        "test_loaders_list = train_loaders # Use the created train loaders for testing as well for simplicity\n",
        "\n",
        "model = Net(num_classes).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "ewc_memory = []\n",
        "ewc_enabled = True\n",
        "\n",
        "for i, train_loader in enumerate(train_loaders):\n",
        "    print(f\"Training on Task {i + 1}\")\n",
        "    if i == 0:\n",
        "        train(model, train_loader, optimizer, criterion, 100, NSAMPLES, use_ewc=False)\n",
        "        temp_model = copy.deepcopy(model)\n",
        "        saved_models_list.append(temp_model)\n",
        "        # prior = prior_distribution(model)\n",
        "    else:\n",
        "        train(model, train_loader, optimizer, criterion, 100, NSAMPLES, use_ewc=ewc_enabled, ewc=ewc_memory)\n",
        "        temp_model = copy.deepcopy(model)\n",
        "        saved_models_list.append(temp_model)\n",
        "    # Store EWC data\n",
        "    ewc_memory.append(EWC(copy.deepcopy(model), train_loader))\n",
        "\n",
        "    # else:\n",
        "    #     train_with_pacbayes(model, train_loader, optimizer, criterion, 100, NSAMPLES, prior)\n",
        "    #     prior = prior_distribution(model) # Update prior after each task\n",
        "    # temp_model = copy.deepcopy(model)\n",
        "    # saved_models_list.append(temp_model)\n",
        "\n",
        "    print(f\"Evaluation after training on Task {i + 1}\")\n",
        "    for j, test_loader in enumerate(test_loaders_list):\n",
        "        acc = evaluate(model, test_loader)\n",
        "        print(f\"  Accuracy on Task {j + 1}: {acc:.2f}%\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.linalg import svdvals\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cross_decomposition import CCA\n",
        "import seaborn as sns\n",
        "\n",
        "# ASSUMPTIONS (define these in your notebook before running):\n",
        "train_loader_task1 = train_loaders[0] if train_loaders else None\n",
        "train_loader_ewc = train_loader_task1\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "device = 'cuda' or 'cpu'\n",
        "\n",
        "# Move all models to CPU and eval mode for analysis\n",
        "for m in saved_models_list:\n",
        "    m.to('cpu').eval()\n",
        "\n",
        "# 1+2+3: weight norms, effective rank, parameter‐space trajectory\n",
        "layers = [('fc1', lambda m: m.net[0]), ('fc2', lambda m: m.net[2])]\n",
        "weights = {name: [] for name, _ in layers}\n",
        "for model in saved_models_list:\n",
        "    for name, getm in layers:\n",
        "        W = getm(model).weight.detach().cpu().numpy()\n",
        "        weights[name].append(W)\n",
        "\n",
        "# Compute metrics\n",
        "norms = {name: [] for name, _ in layers}\n",
        "eranks = {name: [] for name, _ in layers}\n",
        "traj_lengths = {name: 0.0 for name, _ in layers}\n",
        "for name in norms:\n",
        "    for i in range(len(weights[name]) - 1):\n",
        "        dW = weights[name][i+1] - weights[name][i]\n",
        "        norms[name].append(np.linalg.norm(dW))\n",
        "        traj_lengths[name] += np.linalg.norm(dW)\n",
        "    for W in weights[name]:\n",
        "        sv = np.linalg.svd(W, compute_uv=False)\n",
        "        p = sv / sv.sum()\n",
        "        eranks[name].append(np.exp(-np.sum(p * np.log(p + 1e-12))))\n",
        "\n",
        "# 4: EWC Fisher diag on first model\n",
        "def compute_fisher(model, dataloader, criterion):\n",
        "    fishers = {n: torch.zeros_like(p) for n,p in model.named_parameters()}\n",
        "    for imgs, labels in dataloader:\n",
        "        model.zero_grad()\n",
        "        out = model(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "        loss.backward()\n",
        "        for n, p in model.named_parameters():\n",
        "            fishers[n] += p.grad.data.pow(2)\n",
        "    for n in fishers:\n",
        "        fishers[n] /= len(dataloader)\n",
        "    return fishers\n",
        "\n",
        "fisher = compute_fisher(saved_models_list[0].to('cpu'), train_loader_task1, criterion) if train_loader_task1 else None\n",
        "ewc_overlap = []\n",
        "if fisher is not None:\n",
        "    for i in range(len(saved_models_list)-1):\n",
        "        model_prev = saved_models_list[i]\n",
        "        model_next = saved_models_list[i+1]\n",
        "        overlap = 0.0\n",
        "        for n, p in model_prev.named_parameters():\n",
        "            delta = (model_next.state_dict()[n] - p.data).cpu()\n",
        "            overlap += (fisher[n] * delta.pow(2)).sum().item()\n",
        "        ewc_overlap.append(overlap)\n",
        "\n",
        "# 5: SVCCA and 6: canonical angles\n",
        "def svcca(X, Y, max_components=20):\n",
        "    \"\"\"\n",
        "    Mean canonical correlation after PCA to L dims,\n",
        "    where L = min(n_samples, n_features, max_components).\n",
        "    \"\"\"\n",
        "    n_samples, n_features = X.shape\n",
        "    L = min(n_samples, n_features, max_components)\n",
        "    # 1) PCA-reduce\n",
        "    Xr = PCA(n_components=L).fit_transform(X)\n",
        "    Yr = PCA(n_components=L).fit_transform(Y)\n",
        "    # 2) CCA on reduced dims\n",
        "    cca = CCA(n_components=L)\n",
        "    Xc, Yc = cca.fit_transform(Xr, Yr)\n",
        "    # 3) average corr per component\n",
        "    corrs = [np.corrcoef(Xc[:, i], Yc[:, i])[0, 1] for i in range(L)]\n",
        "    return np.mean(corrs)\n",
        "\n",
        "# ---- canonical‐angles helper ----\n",
        "def canonical_angles(X, Y):\n",
        "    \"\"\"\n",
        "    Principal angles between row-spaces of X and Y:\n",
        "    angles = arccos(singular_values(X^T Y)).\n",
        "    \"\"\"\n",
        "    M = X.T.dot(Y)\n",
        "    s = np.linalg.svd(M, compute_uv=False)\n",
        "    # clamp to [-1,1] to avoid numerical errors outside domain\n",
        "    s = np.clip(s, -1.0, 1.0)\n",
        "    angles = np.arccos(s)\n",
        "    return angles\n",
        "\n",
        "\n",
        "# Activation extraction helper\n",
        "def extract_acts(model, loader):\n",
        "    acts = {name: [] for name, _ in layers}\n",
        "    labels = []\n",
        "    hooks = []\n",
        "    for name, getm in layers:\n",
        "        hooks.append(getm(model).register_forward_hook(\n",
        "            lambda m, inp, out, n=name: acts[n].append(out.detach().numpy())\n",
        "        ))\n",
        "    for imgs, lbls in loader:\n",
        "        labels.append(lbls.numpy())\n",
        "        _ = model(imgs)\n",
        "    for h in hooks: h.remove()\n",
        "    # concatenate\n",
        "    for n in acts:\n",
        "        acts[n] = np.concatenate([a.reshape(a.shape[0], -1) for a in acts[n]], axis=0)\n",
        "    labels = np.concatenate(labels, axis=0)\n",
        "    return acts, labels\n",
        "\n",
        "# Precompute activations for all models & loaders\n",
        "acts = {}\n",
        "for mi, model in enumerate(saved_models_list):\n",
        "    acts[mi] = {}\n",
        "    for ti, loader in enumerate(test_loaders_list):\n",
        "        a, lbl = extract_acts(model, loader)\n",
        "        acts[mi][ti] = (a, lbl)\n",
        "\n",
        "# 7: manifold‐geometry & 8: cluster separability\n",
        "def manifold_and_sep(X, labels):\n",
        "    classes = np.unique(labels)\n",
        "    centroids = {}\n",
        "    radii = {}\n",
        "    dims = {}\n",
        "    for c in classes:\n",
        "        Xi = X[labels==c]\n",
        "        cent = Xi.mean(0)\n",
        "        centroids[c] = cent\n",
        "        radii[c] = np.linalg.norm(Xi-cent, axis=1).mean()\n",
        "        pca = PCA().fit(Xi)\n",
        "        cum = np.cumsum(pca.explained_variance_ratio_)\n",
        "        dims[c] = np.searchsorted(cum, 0.9)+1\n",
        "    # separability\n",
        "    mu = X.mean(0)\n",
        "    SW = np.zeros((X.shape[1],X.shape[1]))\n",
        "    SB = np.zeros_like(SW)\n",
        "    for c in classes:\n",
        "        Xi = X[labels==c]\n",
        "        mu_c = centroids[c]\n",
        "        SW += (Xi-mu_c).T @ (Xi-mu_c)\n",
        "        n_c = Xi.shape[0]\n",
        "        diff = mu_c - mu\n",
        "        SB += n_c * np.outer(diff, diff)\n",
        "    sep = np.trace(SB)/np.trace(SW)\n",
        "    return centroids, radii, dims, sep\n",
        "\n",
        "manifold_metrics = {}\n",
        "for mi in acts:                      # model index\n",
        "    manifold_metrics[mi] = {}\n",
        "    for ti in acts[mi]:              # task/input index\n",
        "        manifold_metrics[mi][ti] = {}\n",
        "        acts_dict, labels = acts[mi][ti]\n",
        "        # unpack name, getter from layers\n",
        "        for name, _ in layers:\n",
        "            X   = acts_dict[name]   # (N, features)\n",
        "            lbl = labels            # (N,)\n",
        "            # compute centroids\n",
        "            manifold_metrics[mi][ti][name] = manifold_and_sep(X, lbl)\n",
        "\n",
        "\n",
        "# 9: representational drift (CKA) relative to first model on task1 inputs\n",
        "def linear_CKA(X, Y):\n",
        "    Xc = X - X.mean(0)\n",
        "    Yc = Y - Y.mean(0)\n",
        "    HSIC = np.linalg.norm(Xc.T.dot(Yc), 'fro')**2\n",
        "    denom = np.linalg.norm(Xc.T.dot(Xc), 'fro') * np.linalg.norm(Yc.T.dot(Yc), 'fro')\n",
        "    return HSIC / denom\n",
        "\n",
        "drift = {name: [] for name, _ in layers}\n",
        "if 0 in acts and 0 in acts[0]:\n",
        "    for layer_name, _ in layers:\n",
        "        if layer_name in acts[0][0][0].keys():  # Correct way to check for keys\n",
        "            base_X = acts[0][0][0][layer_name]  # model 0, loader 0\n",
        "            for mi in range(1, len(saved_models_list)):\n",
        "                if mi in acts and 0 in acts[mi]:\n",
        "                    acts_dict_mi, _ = acts[mi][0]\n",
        "                    if layer_name in acts_dict_mi:\n",
        "                        X = acts_dict_mi[layer_name]\n",
        "                        drift[layer_name].append(linear_CKA(base_X, X))\n",
        "\n",
        "# 10: inter‐task transfer (accuracy matrix)\n",
        "acc_matrix = np.zeros((len(saved_models_list), len(test_loaders_list)))\n",
        "for mi, model in enumerate(saved_models_list):\n",
        "    for ti, loader in enumerate(test_loaders_list):\n",
        "        correct = total = 0\n",
        "        for imgs, lbls in loader:\n",
        "            out = model(imgs)\n",
        "            pred = out.argmax(1)\n",
        "            correct += (pred == lbls).sum().item()\n",
        "            total += lbls.size(0)\n",
        "        acc_matrix[mi, ti] = 100 * correct / total\n",
        "\n",
        "# --- VISUALIZATIONS ---\n",
        "# Weight norms & effective rank\n",
        "for name in norms:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(norms[name], marker='o')\n",
        "    plt.title(f'{name} update norms')\n",
        "    plt.xlabel('Update Step')\n",
        "    plt.ylabel('Norm')\n",
        "    plt.xticks(range(len(norms[name])))  # Show all task transitions\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(eranks[name], marker='o')\n",
        "    plt.title(f'{name} effective rank')\n",
        "    plt.xlabel('Task Index')\n",
        "    plt.ylabel('Effective Rank')\n",
        "    plt.xticks(range(len(eranks[name])))  # Show all tasks\n",
        "    plt.suptitle(f'Layer {name}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# EWC overlap\n",
        "if ewc_overlap:\n",
        "    plt.figure()\n",
        "    plt.plot(ewc_overlap, marker='o')\n",
        "    plt.title('EWC importance overlap per update')\n",
        "    plt.xlabel('Update Step')\n",
        "    plt.ylabel('Overlap')\n",
        "    plt.xticks(range(len(ewc_overlap)))  # Show all task transitions\n",
        "    plt.show()\n",
        "\n",
        "# SVCCA & angles\n",
        "for name, _ in layers:\n",
        "    sv_vals, ang_vals = [], []\n",
        "    # baseline activations: model 0 on loader 0\n",
        "    if 0 in acts and 0 in acts[0]:\n",
        "        base_acts, _ = acts[0][0]     # acts[model_idx][loader_idx] = (dict, labels)\n",
        "        if name in base_acts:\n",
        "            X0 = base_acts[name]         # shape = (n_samples, feature_dim)\n",
        "\n",
        "            for mi in range(1, len(saved_models_list)):\n",
        "                if mi in acts and 0 in acts[mi]:\n",
        "                    comp_acts, _ = acts[mi][0]\n",
        "                    if name in comp_acts:\n",
        "                        Xi = comp_acts[name]\n",
        "                        sv_vals.append(svcca(X0, Xi))\n",
        "                        ang_vals.append(canonical_angles(X0, Xi).mean())\n",
        "\n",
        "            # plot side by side\n",
        "            fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "            axs[0].plot(sv_vals, marker='o')\n",
        "            axs[0].set_title(f'SVCCA similarity — {name}')\n",
        "            axs[0].set_xlabel('Model Index (vs Model 0)')\n",
        "            axs[0].set_ylabel('Mean Canonical Corr')\n",
        "            axs[0].set_xticks(range(len(sv_vals)))  # Show all model transitions\n",
        "\n",
        "            axs[1].plot(ang_vals, marker='o')\n",
        "            axs[1].set_title(f'Mean canonical angle — {name}')\n",
        "            axs[1].set_xlabel('Model Index (vs Model 0)')\n",
        "            axs[1].set_ylabel('Angle (rad)')\n",
        "            axs[1].set_xticks(range(len(ang_vals)))  # Show all model transitions\n",
        "\n",
        "            plt.suptitle(f'Representation similarity for layer \"{name}\"', y=1.02)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "# Manifold & separability for model 0 on task1 inputs\n",
        "if 0 in manifold_metrics and 0 in manifold_metrics[0] and 'fc1' in manifold_metrics[0][0]:\n",
        "    cent, rad, dims, sep = manifold_metrics[0][0]['fc1']\n",
        "    print('Class radii (fc1):', rad)\n",
        "    print('Manifold dims (fc1):', dims)\n",
        "    print('Cluster separability (fc1):', sep)\n",
        "    if 'fc2' in manifold_metrics[0][0]:\n",
        "        cent_fc2, rad_fc2, dims_fc2, sep_fc2 = manifold_metrics[0][0]['fc2']\n",
        "        print('Class radii (fc2):', rad_fc2)\n",
        "        print('Manifold dims (fc2):', dims_fc2)\n",
        "        print('Cluster separability (fc2):', sep_fc2)\n",
        "\n",
        "# Representational drift\n",
        "for name in drift:\n",
        "    plt.figure()\n",
        "    plt.plot(drift[name], marker='o', label=name)\n",
        "    plt.title(f'CKA-based drift from model0 {name} outputs')\n",
        "    plt.xlabel('Model Index')\n",
        "    plt.ylabel('CKA Similarity')\n",
        "    plt.xticks(range(len(drift[name])))  # Show all model transitions\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Inter-task transfer heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(acc_matrix, annot=True, fmt=\".1f\", cmap=\"viridis\",\n",
        "            xticklabels=[f'Task {j+1}' for j in range(acc_matrix.shape[1])],\n",
        "            yticklabels=[f'Model {i+1}' for i in range(acc_matrix.shape[0])])\n",
        "plt.xlabel('Test Task'); plt.ylabel('Model Trained On')\n",
        "plt.title('Inter-task Transfer Accuracy (%)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4shsO8RfJ7kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PARAMETER SPACE TRAJECTORY VISUALIZATION ---\n",
        "def plot_weight_trajectory(weight_list, layer_name, ndim=2):\n",
        "    \"\"\"\n",
        "    Project trajectory of weights into 2D or 3D using PCA.\n",
        "    \"\"\"\n",
        "    flattened = [w.flatten() for w in weight_list]\n",
        "    X = np.stack(flattened)  # Shape: (num_steps, num_weights)\n",
        "    pca = PCA(n_components=ndim)\n",
        "    X_proj = pca.fit_transform(X)\n",
        "\n",
        "    fig = plt.figure(figsize=(6, 6))\n",
        "    if ndim == 3:\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        ax.plot(X_proj[:, 0], X_proj[:, 1], X_proj[:, 2], marker='o')\n",
        "        ax.set_xlabel('PC1'); ax.set_ylabel('PC2'); ax.set_zlabel('PC3')\n",
        "    else:\n",
        "        ax = fig.add_subplot(111)\n",
        "        ax.plot(X_proj[:, 0], X_proj[:, 1], marker='o')\n",
        "        for i, (x, y) in enumerate(zip(X_proj[:, 0], X_proj[:, 1])):\n",
        "            ax.text(x, y, str(i), fontsize=8)\n",
        "        ax.set_xlabel('PC1'); ax.set_ylabel('PC2')\n",
        "\n",
        "    plt.title(f'Parameter Trajectory: {layer_name}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot trajectories\n",
        "for name in weights:\n",
        "    plot_weight_trajectory(weights[name], name, ndim=2)  # Set ndim=3 for 3D if preferred\n"
      ],
      "metadata": {
        "id": "pQHh1BWNResO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-GLHcZPcsLPV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}